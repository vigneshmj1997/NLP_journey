{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch \n",
    "! pip install transformers\n",
    "! pip install datasets\n",
    "! pip install evaluate\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "from datasets import Value, load_dataset\n",
    "raw_datasets=load_dataset(\"app_reviews\",split='train[:20%]').train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=set(raw_datasets[\"train\"][\"star\"])\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig,AutoModelForSequenceClassification,AutoTokenizer\n",
    "model_name=\"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "        \"bert-base-uncased\", #feel free to use other models\n",
    "        num_labels=len(label_list),\n",
    "        finetuning_task=\"text-classification\"\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"bert-base-uncased\"\n",
    "    )\n",
    "model.config.label2id = label_to_id\n",
    "model.config.id2label = {id: label for label, id in config.label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):   \n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[\"review\"], padding=False, max_length=512, truncation=True)\n",
    "    \n",
    "    result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"star\"]]\n",
    "    print(result[\"label\"])    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn\n",
    "import evaluate\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "training_args=TrainingArguments(output_dir=\"temp\")\n",
    "train_dataset=raw_datasets[\"train\"]\n",
    "eval_dataset=raw_datasets[\"test\"]\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset ,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "#train_result = trainer.train(resume_from_checkpoint=checkpoint) if you wanna resume\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = (len(train_dataset))\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "max_eval_samples = len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "pipe=Pipeline(\"text-classification\",model=\"./demo/\")\n",
    "pipe(input(\"Enter the text here\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
